[tool:pytest]
# Core pytest configuration for plume navigation simulation testing
# Minimum pytest version requirement for scientific computing features
minversion = 8.3.5

# Additional command line options for comprehensive testing
addopts = 
    -ra 
    -q 
    --strict-markers 
    --strict-config 
    --tb=short
    --durations=10
    --durations-min=1.0
    --capture=sys
    --showlocals=false
    --verbose
    --color=yes

# Test discovery paths organized by test category
testpaths = 
    unit
    integration
    performance

# Python file patterns for test discovery
python_files = 
    test_*.py
    *_test.py

# Python class patterns for test discovery
python_classes = Test*

# Python function patterns for test discovery  
python_functions = test_*

# Custom test markers for categorization and filtering
markers =
    unit: Unit tests for individual components
    integration: Integration tests for component interaction
    performance: Performance tests for speed and resource validation
    slow: Tests that take longer than 10 seconds
    crimaldi: Tests specific to Crimaldi format processing
    custom_format: Tests specific to custom AVI format processing
    cross_format: Tests validating cross-format compatibility
    batch_processing: Tests for batch simulation execution
    accuracy: Tests validating >95% correlation requirements
    scientific_computing: Tests requiring numerical precision validation
    mock: Tests using mock components and data
    regression: Regression tests for performance and accuracy
    stress: Stress tests for system limits and robustness
    numerical_precision: Tests requiring high numerical precision
    format_conversion: Tests for data format conversion
    parallel_execution: Tests for parallel processing validation
    memory_intensive: Tests requiring significant memory resources
    benchmark: Performance benchmark tests

# Warning filters for scientific computing environment
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning:numpy.*
    ignore::PendingDeprecationWarning
    ignore::RuntimeWarning:scipy.*
    ignore::FutureWarning:pandas.*
    ignore::ImportWarning
    ignore::ResourceWarning

# Console logging configuration for real-time monitoring
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File logging configuration for detailed debugging
log_file = logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# Output styling for enhanced readability
console_output_style = progress

# JUnit XML configuration for CI/CD integration
junit_family = xunit2
junit_logging = all
junit_log_passing_tests = false

# Timeout configuration for scientific computing tests
timeout = 300
timeout_method = thread

# Asyncio configuration for asynchronous test support
asyncio_mode = auto

# Cache configuration for improved test performance
cache_dir = .pytest_cache

# Collection ignore patterns to exclude non-test files
collect_ignore = 
    setup.py
    conftest.py

# Directory patterns to exclude from test collection
norecursedirs = 
    .*
    build
    dist
    CVS
    _darcs
    {arch}
    *.egg
    venv
    env
    __pycache__
    .pytest_cache

# Doctest configuration for scientific computing validation
doctest_optionflags = 
    NORMALIZE_WHITESPACE
    IGNORE_EXCEPTION_DETAIL
    ALLOW_UNICODE
    ELLIPSIS

doctest_encoding = utf-8

# Test execution configuration
maxfail = 10
tb_style = short
show_capture = all

# Collection and execution behavior
continue_on_collection_errors = false
disable_warnings = false
error_for_skips = false

# Environment variables for scientific computing
# Set through environment configuration in conftest.py
addopts = --import-mode=importlib

# Plugin-specific configuration sections
[coverage:run]
# Coverage analysis configuration for code quality assurance
source = 
    src/backend
    src/test

omit = 
    */test_*
    */conftest.py
    */setup.py
    */__init__.py
    */venv/*
    */env/*

include = 
    src/backend/*
    src/test/utils/*
    src/test/mocks/*

parallel = true
branch = true
data_file = .coverage

[coverage:report]
# Coverage reporting configuration
show_missing = true
skip_covered = false
precision = 2
fail_under = 95.0

exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    @abstract

[coverage:html]
# HTML coverage report configuration
directory = htmlcov
title = Plume Navigation Simulation Coverage Report

[coverage:xml]
# XML coverage report configuration for CI/CD
output = coverage.xml

[coverage:json]
# JSON coverage report configuration
output = coverage.json

[pytest-benchmark]
# Performance benchmarking configuration
min-rounds = 5
max-time = 1.0
min-time = 0.000005
timer = perf_counter
calibration-precision = 10
warmup = true
warmup-iterations = 100000
disable-gc = true
sort = mean
json = benchmark.json
compare-fail = mean:5%
histogram = true

[pytest-html]
# HTML test report configuration
self-contained-html = true
title = Plume Navigation Simulation Test Report
css = 
    assets/style.css

[pytest-json-report]
# JSON test report configuration
jsonpath = test_report.json
summary = true
stream = false

[pytest-xdist]
# Parallel execution configuration
workers = auto
dist = loadscope
tx = --timeout=300
rsyncdir = src
rsyncignore = 
    *.pyc
    __pycache__
    .git
    .pytest_cache
    .coverage*

# Scientific computing environment configuration
[pytest-env]
# Environment variables for reproducible scientific computing
PYTHONDONTWRITEBYTECODE = 1
PYTHONUNBUFFERED = 1
PYTEST_CURRENT_TEST = 1
NUMBA_DISABLE_JIT = 1
OMP_NUM_THREADS = 1
NUMPY_EXPERIMENTAL_ARRAY_FUNCTION = 0
SCIPY_DISABLE_CUDA = 1

# Mock configuration for testing
[pytest-mock]
mock-use-standalone-module = true
mock-patch-multiple = true
mock-autospec = true
mock-spec-set = true

# Timeout configuration for different test categories
[pytest-timeout]
timeout = 300
timeout_method = thread
timeout_func_only = false