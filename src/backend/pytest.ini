# Pytest configuration file for the plume navigation simulation backend system
# Defines comprehensive testing framework settings optimized for scientific computing validation
# Configures test discovery, execution parameters, performance benchmarking, coverage requirements,
# parallel processing, and cross-format compatibility testing to ensure >95% correlation accuracy,
# <7.2 seconds per simulation performance, and reproducible scientific results across 4000+ batch simulations

[pytest]
# Minimum pytest version required for backend scientific computing features
minversion = 8.3.5

# Command line options applied to all test runs
# -ra: show extra test summary info for all except passed
# -q: quiet mode to reduce output verbosity
# --strict-markers: treat unregistered markers as errors
# --strict-config: treat configuration errors as failures
# --tb=short: short traceback format for better readability
# --maxfail=5: stop after 5 failures to prevent cascading errors
# --durations=10: show slowest 10 test durations for performance monitoring
addopts = -ra -q --strict-markers --strict-config --tb=short --maxfail=5 --durations=10

# Test discovery paths for backend components
testpaths = 
    src/backend/core
    src/backend/utils
    src/backend/algorithms
    src/backend/io
    src/backend/cache
    src/backend/monitoring
    src/backend/error

# Python file patterns for test discovery
python_files = 
    test_*.py
    *_test.py

# Python class patterns for test discovery
python_classes = 
    Test*
    *Test
    *TestSuite

# Python function patterns for test discovery
python_functions = test_*

# Custom markers for backend-specific test categorization
markers =
    unit: Unit tests for individual backend components
    integration: Integration tests for backend component interaction
    performance: Performance tests for backend speed and resource validation
    slow: Backend tests that take longer than 10 seconds
    fast: Backend tests that complete within 2 seconds
    crimaldi: Tests specific to Crimaldi format processing in backend
    custom_format: Tests specific to custom AVI format processing in backend
    cross_format: Tests validating backend cross-format compatibility
    batch_processing: Tests for backend batch simulation execution
    accuracy: Tests validating backend >95% correlation requirements
    scientific_computing: Tests requiring backend numerical precision validation
    algorithms: Tests for navigation algorithm implementations
    data_normalization: Tests for backend data normalization pipeline
    simulation_engine: Tests for backend simulation execution engine
    analysis_pipeline: Tests for backend performance analysis components
    io_operations: Tests for backend input/output operations
    caching: Tests for backend caching mechanisms
    monitoring: Tests for backend performance monitoring
    error_handling: Tests for backend error detection and recovery
    parallel_processing: Tests for backend parallel execution capabilities
    memory_management: Tests for backend memory optimization
    reproducibility: Tests validating backend >0.99 reproducibility coefficient
    regression: Regression tests for backend performance and accuracy
    stress: Stress tests for backend system limits and robustness

# Warning filters to manage scientific computing library warnings
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning:numpy.*
    ignore::PendingDeprecationWarning
    ignore::RuntimeWarning:scipy.*
    ignore::FutureWarning:pandas.*

# Live logging configuration for real-time test monitoring
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File logging configuration for detailed test analysis
log_file = logs/backend_pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s (%(filename)s:%(lineno)d)
log_file_date_format = %Y-%m-%d %H:%M:%S

# Console output styling for better readability
console_output_style = progress

# JUnit XML reporting configuration for CI/CD integration
junit_family = xunit2
junit_logging = all
junit_log_passing_tests = false

[tool:pytest]
# Extended pytest configuration for backend scientific computing and performance testing

# Timeout configuration for long-running scientific computations
timeout = 300
timeout_method = thread

# Asyncio mode for concurrent operations
asyncio_mode = auto

# Backend-specific cache directory
cache_dir = .pytest_cache/backend

# Files and directories to ignore during collection
collect_ignore = 
    setup.py
    conftest.py
    examples
    scripts

# Directories to avoid during recursive test discovery
norecursedirs = 
    .*
    build
    dist
    CVS
    _darcs
    {arch}
    *.egg
    venv
    env
    examples
    scripts

# Doctest configuration for scientific computing validation
doctest_optionflags = 
    NORMALIZE_WHITESPACE
    IGNORE_EXCEPTION_DETAIL
    ALLOW_UNICODE

# Doctest encoding for cross-platform compatibility
doctest_encoding = utf-8

[tool:coverage:run]
# Coverage configuration for backend source code analysis
source = src/backend

# Files to omit from coverage analysis
omit = 
    */test_*
    */conftest.py
    */setup.py
    */__init__.py
    */examples/*
    */scripts/*

# Specific backend modules to include in coverage
include = 
    src/backend/core/*
    src/backend/utils/*
    src/backend/algorithms/*
    src/backend/io/*
    src/backend/cache/*
    src/backend/monitoring/*
    src/backend/error/*

[tool:coverage:report]
# Coverage reporting configuration
exclude_lines = 
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:

# Coverage display options
show_missing = true
skip_covered = false
precision = 2
fail_under = 90.0

[tool:coverage:html]
# HTML coverage report configuration
directory = htmlcov/backend

[tool:coverage:xml]
# XML coverage report configuration
output = coverage_backend.xml

[tool:coverage:json]
# JSON coverage report configuration
output = coverage_backend.json

[tool:pytest-xdist]
# Parallel execution configuration for scientific computing workloads
# Auto-detect number of workers based on CPU cores
workers = auto

# Load balancing and distribution mode
distribution_mode = loadscope
load_balancing = true

# Files to ignore during rsync for distributed testing
rsync_ignore = 
    *.pyc
    __pycache__
    .git
    .pytest_cache
    examples
    scripts

# Process isolation settings
boxed = false
forked = false

# Resource management for parallel execution
max_worker_restart = 3
worker_timeout = 300
collect_timeout = 60
setup_timeout = 30

[tool:pytest-benchmark]
# Benchmark configuration for performance validation
min_rounds = 5
max_time = 1.0
min_time = 0.000005
timer = perf_counter
calibration_precision = 10
warmup = true
warmup_iterations = 100000
disable_gc = true
sort = mean

# Backend-specific performance thresholds
json = benchmark_backend.json
compare = benchmark_compare_backend.json

[tool:pytest-html]
# HTML test report configuration
title = Plume Navigation Backend Test Report
report = report_backend.html

[tool:pytest-json-report]
# JSON test report configuration
jsonpath = test_report_backend.json

[tool:pytest-mock]
# Mock configuration for backend testing
mock_use_standalone_module = true

# Environment variables for backend testing
[pytest]
# Python path configuration for backend modules
pythonpath = 
    src
    src/backend

# Backend-specific environment variables
env = 
    PYTHONDONTWRITEBYTECODE = 1
    PYTHONUNBUFFERED = 1
    PYTEST_CURRENT_TEST = 1
    NUMBA_DISABLE_JIT = 1
    OMP_NUM_THREADS = 1
    BACKEND_TEST_MODE = 1
    SCIENTIFIC_COMPUTING_MODE = 1

# Numerical precision configuration for scientific computing
NUMERICAL_TOLERANCE = 1e-6
CORRELATION_THRESHOLD = 0.95
REPRODUCIBILITY_COEFFICIENT = 0.99
FLOATING_POINT_PRECISION = float64
RANDOM_SEED = 42
NUMPY_ERROR_HANDLING = raise

# Performance requirements configuration
SIMULATION_TIME_LIMIT = 7.2
BATCH_COMPLETION_TARGET = 8_hours_for_4000_simulations
MEMORY_EFFICIENCY = optimized_for_backend_processing
PARALLEL_PROCESSING = pytest_xdist_based
BENCHMARK_ITERATIONS = 10
WARMUP_ITERATIONS = 3

# Backend performance thresholds
SIMULATION_TIME_THRESHOLD = 7.2
NORMALIZATION_TIME_THRESHOLD = 0.72
ANALYSIS_TIME_THRESHOLD = 0.36
MEMORY_THRESHOLD_MB = 1024
CPU_THRESHOLD_PERCENT = 80
ALGORITHM_EXECUTION_THRESHOLD = 5.0
IO_OPERATION_THRESHOLD = 2.0
CACHE_OPERATION_THRESHOLD = 0.1