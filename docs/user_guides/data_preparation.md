# Data Preparation Guide

## Overview

### Introduction to Data Preparation

The data preparation pipeline for plume simulation analysis is a critical component that ensures accurate, consistent, and reproducible processing of plume recording data. This comprehensive system handles the normalization and calibration of plume recordings across different physical scales, formats, and experimental conditions to achieve >95% correlation with reference implementations while supporting 4000+ simulation processing requirements.

The data preparation process transforms raw video recordings from various sources (Crimaldi dataset and custom AVI recordings) into standardized, normalized datasets suitable for scientific computing and algorithm validation. This normalization is essential for meaningful comparison of navigation algorithm performance across different experimental conditions and ensures scientific reproducibility.

Key capabilities include:
- Automated format detection and compatibility validation
- Physical scale normalization across different arena sizes and resolutions
- Temporal synchronization and frame rate standardization
- Intensity calibration and unit conversion
- Comprehensive quality assurance and validation
- Fail-fast error detection and recovery mechanisms

### Supported Data Formats

The system supports two primary data formats with automatic detection and cross-format compatibility:

**Crimaldi Dataset Format:**
- Standard research format with established calibration parameters
- Pixel-to-meter ratio: 100.0 pixels/meter
- Intensity units: concentration_ppm
- Coordinate system: cartesian
- Frame rate: 50.0 Hz
- Bit depth: 16-bit
- Comprehensive metadata for automated calibration

**Custom AVI Format:**
- Flexible format for custom experimental setups
- Supported codecs: H264, MJPEG, uncompressed
- Pixel-to-meter ratio: 150.0 pixels/meter (configurable)
- Intensity units: raw_sensor
- Coordinate system: image
- Frame rate: 30.0 Hz (configurable)
- Bit depth: 8-bit
- Auto-detection with metadata extraction

The system provides seamless cross-format compatibility with automated conversion matrices and tolerance thresholds (spatial accuracy: 0.01, temporal accuracy: 0.001, intensity accuracy: 0.02) to ensure consistent processing across different data sources.

### Data Preparation Workflow

The data preparation workflow follows a systematic approach designed for scientific computing precision and reproducibility:

1. **Format Detection and Validation**
   - Automatic format detection with confidence scoring
   - Compatibility validation against target requirements
   - Metadata extraction and parameter verification

2. **File Integrity Validation**
   - Video file accessibility and corruption detection
   - Frame count consistency verification
   - Deep validation with integrity checks

3. **Physical Scale Normalization**
   - Arena dimension standardization (target: 1.0m x 1.0m)
   - Pixel resolution normalization (target: 640x480)
   - Temporal alignment and frame rate conversion
   - Intensity calibration and range optimization

4. **Quality Assurance and Validation**
   - Comprehensive validation pipeline execution
   - Quality metrics calculation and threshold verification
   - Cross-format consistency validation
   - Performance validation and optimization

5. **Output Generation and Reporting**
   - Normalized data output in standardized format
   - Quality assessment reports and metrics
   - Processing logs and audit trails
   - Error analysis and recovery recommendations

## Prerequisites and Setup

### System Requirements

**Hardware Requirements:**
- Memory: 8GB RAM recommended (minimum 4GB)
- Storage: 50GB available space for processing and caching
- CPU: Multi-core processor for parallel processing optimization
- Operating System: Windows 10+, macOS 10.15+, or Linux (Ubuntu 18.04+)

**Performance Considerations:**
- Processing capability for 4000+ simulations within 8 hours
- Parallel processing support for batch operations
- Memory mapping capabilities for large file handling
- Network storage compatibility for distributed datasets

### Installation and Configuration

**Required Dependencies:**
Install the following Python packages with specified versions:

```bash
# Core scientific computing libraries
pip install numpy==2.1.3  # Numerical computation and array processing
pip install scipy==1.15.3  # Scientific computing and statistical functions
pip install opencv-python==4.11.0  # Video processing and computer vision
pip install pandas==2.2.0  # Data manipulation and analysis

# Parallel processing and optimization
pip install joblib==1.6.0  # Parallel computing and memory mapping
pip install matplotlib==3.9.0  # Visualization and plotting
pip install seaborn==0.13.2  # Statistical visualization

# Testing and quality assurance
pip install pytest==8.3.5  # Testing framework for validation
```

**Configuration Setup:**
1. Create configuration directory: `config/`
2. Copy default configuration: `config/default_normalization.json`
3. Set environment variables for cache and temporary directories
4. Configure memory limits and parallel processing parameters
5. Validate installation using provided test scripts

### Environment Validation

**Installation Verification:**
Run the following validation procedures to ensure proper setup:

```bash
# Validate core dependencies
python -c "import numpy, scipy, cv2, pandas, joblib; print('Dependencies OK')"

# Check processing capabilities
python scripts/validate_environment.py

# Run basic functionality tests
pytest tests/test_environment_validation.py -v

# Performance benchmark
python scripts/benchmark_processing.py
```

**Configuration Validation:**
- Verify configuration file structure and parameters
- Test video format detection capabilities
- Validate memory allocation and parallel processing
- Confirm cache directory accessibility and permissions

## Data Format Requirements

### Crimaldi Format Specifications

The Crimaldi dataset format represents the standard reference format for plume simulation research with the following specifications:

**Technical Parameters:**
- **Pixel-to-meter ratio:** 100.0 pixels/meter (fixed standard)
- **Intensity units:** concentration_ppm (parts per million concentration)
- **Coordinate system:** cartesian (standard scientific coordinate system)
- **Frame rate:** 50.0 Hz (high temporal resolution for detailed analysis)
- **Bit depth:** 16-bit (high dynamic range for concentration measurements)
- **Arena dimensions:** Standardized to 1.0m x 1.0m for consistency

**Metadata Requirements:**
- Calibration parameters embedded in video metadata
- Experimental conditions documentation
- Camera calibration information
- Environmental parameters (temperature, humidity, wind conditions)
- Temporal synchronization markers

**File Structure:**
- Standard AVI container with uncompressed frames
- Metadata stored in AVI INFO chunk
- Frame-accurate timing information
- Consistent color space representation

### Custom AVI Format Requirements

Custom AVI recordings provide flexibility for various experimental setups while maintaining compatibility with the normalization pipeline:

**Supported Codecs:**
- **H264:** Efficient compression for large datasets
- **MJPEG:** Frame-independent encoding for random access
- **Uncompressed:** Maximum quality preservation for critical analyses

**Configurable Parameters:**
- **Pixel-to-meter ratio:** Default 150.0 pixels/meter (adjustable based on setup)
- **Intensity units:** raw_sensor (direct sensor readings)
- **Coordinate system:** image (pixel-based coordinate system)
- **Frame rate:** Default 30.0 Hz (configurable for different cameras)
- **Bit depth:** 8-bit (standard for most camera systems)

**Auto-Detection Capabilities:**
- Automatic codec detection and compatibility assessment
- Metadata extraction from various sources (file headers, embedded data)
- Frame rate and resolution detection
- Calibration parameter inference from available information

### Format Compatibility Matrix

The system provides comprehensive cross-format compatibility through automated conversion and validation:

**Conversion Matrices:**
- Spatial scaling conversion between pixel ratios
- Temporal interpolation for frame rate alignment
- Intensity mapping between different unit systems
- Coordinate system transformation (cartesian â†” image)

**Tolerance Thresholds:**
- **Spatial accuracy:** 0.01 (1% tolerance for geometric measurements)
- **Temporal accuracy:** 0.001 (millisecond-level timing precision)
- **Intensity accuracy:** 0.02 (2% tolerance for concentration measurements)
- **Overall correlation:** >95% with reference implementations

**Compatibility Assessment:**
- Automatic compatibility scoring for format pairs
- Identification of potential conversion limitations
- Recommendations for optimal processing parameters
- Warning system for quality-impacting conversions

## Data Validation Procedures

### Format Detection and Validation

**Automatic Format Detection:**
The system employs sophisticated algorithms to automatically detect and validate video formats:

```python
# Format detection with confidence scoring
format_info = detect_video_format(video_path, deep_inspection=True)
confidence = format_info["confidence"]  # Confidence level (0.0-1.0)
detected_format = format_info["format"]  # "crimaldi" or "custom_avi"
```

**Validation Procedures:**
- Deep inspection of video container and codec information
- Metadata analysis for format-specific markers
- Frame structure analysis for format consistency
- Confidence threshold validation (minimum 0.8 for automatic processing)

**Format Compatibility Validation:**
- Cross-format compatibility assessment
- Conversion feasibility analysis
- Quality impact prediction for format conversions
- Automated parameter suggestion for optimal processing

### File Integrity Validation

**Comprehensive Integrity Checking:**
The validation system performs thorough integrity checks to ensure reliable processing:

```python
# Video integrity validation
video_reader = create_video_reader_factory(video_path, enable_caching=True)
integrity_result = video_reader.validate_integrity(deep_validation=True)
```

**Validation Components:**
- **File accessibility:** Verify file permissions and readability
- **Container integrity:** Validate video container structure
- **Frame consistency:** Check frame count and sequence integrity
- **Metadata validation:** Verify required metadata presence and format
- **Corruption detection:** Identify damaged or incomplete frames

**Deep Validation Options:**
- Frame-by-frame validation for critical applications
- Checksum verification for data integrity
- Temporal consistency validation across frame sequences
- Content analysis for expected data patterns

### Parameter Validation

**Calibration Parameter Validation:**
Critical validation of physical calibration parameters ensures scientific accuracy:

```python
# Validate calibration parameters
calibration_params = {
    "arena_width_meters": 1.0,
    "arena_height_meters": 1.0,
    "pixel_to_meter_ratio": 100.0,
    "frame_rate_hz": 50.0
}

validation_result = validate_physical_calibration_parameters(
    calibration_params=calibration_params,
    source_format="crimaldi",
    target_format="normalized"
)
```

**Parameter Constraints:**
- **Arena dimensions:** MIN_ARENA_SIZE_METERS (0.1m) to MAX_ARENA_SIZE_METERS (10.0m)
- **Pixel ratios:** 10.0 to 1000.0 pixels/meter (physical feasibility limits)
- **Frame rates:** 1.0 to 1000.0 Hz (technical capability limits)
- **Intensity ranges:** Format-specific validation against scientific constraints

**Scientific Validation:**
- Physical feasibility assessment for experimental parameters
- Cross-validation against known reference values
- Consistency checking across related parameters
- Automatic parameter correction suggestions

## Normalization Configuration

### Configuration File Structure

The normalization pipeline is configured through a comprehensive JSON configuration file that defines all processing parameters:

**Default Configuration Structure (`config/default_normalization.json`):**
```json
{
  "normalization_pipeline": {
    "enable_parallel_processing": true,
    "max_workers": 8,
    "memory_limit_gb": 8,
    "enable_caching": true,
    "cache_size_limit_gb": 5,
    "processing_precision": "float64"
  },
  "arena_normalization": {
    "target_arena_width_meters": 1.0,
    "target_arena_height_meters": 1.0,
    "scaling_method": "bicubic",
    "boundary_detection": "edge_detection",
    "edge_threshold": 0.1,
    "validation_accuracy_threshold": 0.01
  },
  "pixel_resolution_normalization": {
    "target_width_pixels": 640,
    "target_height_pixels": 480,
    "resampling_algorithm": "lanczos",
    "anti_aliasing": true,
    "quality_preservation": "high",
    "similarity_threshold": 0.95
  },
  "temporal_normalization": {
    "target_frame_rate_hz": 30.0,
    "interpolation_method": "cubic",
    "frame_alignment": "center",
    "synchronization_method": "cross_correlation",
    "motion_preservation_threshold": 0.95
  },
  "intensity_calibration": {
    "target_intensity_range": [0.0, 1.0],
    "calibration_method": "min_max",
    "background_subtraction": true,
    "noise_reduction": "gaussian",
    "dynamic_range_optimization": true,
    "calibration_accuracy_threshold": 0.02
  }
}
```

### Arena Normalization Settings

**Physical Scale Standardization:**
Arena normalization ensures consistent physical scales across different experimental setups:

- **Target dimensions:** 1.0m x 1.0m (standard reference arena)
- **Scaling method:** Bicubic interpolation for smooth scaling
- **Boundary detection:** Edge detection with configurable threshold (0.1)
- **Calibration markers:** Automatic detection and alignment
- **Validation accuracy:** 0.01 (1% tolerance for geometric accuracy)

**Advanced Configuration Options:**
- Adaptive boundary detection for complex arena shapes
- Multi-point calibration for non-uniform scaling correction
- Perspective correction for camera angle compensation
- Automatic arena detection with confidence scoring

### Pixel Resolution Normalization

**Resolution Standardization:**
Pixel resolution normalization ensures consistent spatial sampling across datasets:

- **Target resolution:** 640x480 pixels (optimal balance of detail and processing speed)
- **Resampling algorithm:** Lanczos interpolation for high-quality scaling
- **Anti-aliasing:** Enabled to prevent visual artifacts
- **Quality preservation:** High-quality settings for scientific accuracy
- **Similarity threshold:** 0.95 (95% similarity requirement with original)

**Quality Control Measures:**
- Pre and post-processing quality assessment
- Automatic detection of quality degradation
- Adaptive algorithm selection based on content analysis
- Preservation of critical spatial features

### Temporal Normalization Configuration

**Frame Rate Standardization:**
Temporal normalization ensures consistent time sampling across different recording systems:

- **Target frame rate:** 30.0 fps (balanced temporal resolution)
- **Interpolation method:** Cubic interpolation for smooth motion
- **Frame alignment:** Center alignment for temporal accuracy
- **Synchronization method:** Cross-correlation for optimal alignment
- **Motion preservation:** 0.95 threshold for movement accuracy

**Advanced Temporal Processing:**
- Adaptive interpolation based on motion content
- Temporal consistency validation across frame sequences
- Motion vector analysis for quality assessment
- Automatic detection of temporal artifacts

### Intensity Calibration Settings

**Intensity Standardization:**
Intensity calibration normalizes concentration measurements across different sensor systems:

- **Target range:** 0.0-1.0 (normalized intensity scale)
- **Calibration method:** Min-max normalization with outlier detection
- **Background subtraction:** Automatic background estimation and removal
- **Noise reduction:** Gaussian filtering for signal enhancement
- **Dynamic range:** Optimization for maximum information preservation
- **Calibration accuracy:** 0.02 (2% tolerance for intensity measurements)

**Scientific Calibration Features:**
- Multi-point calibration using reference standards
- Temporal stability assessment for drift detection
- Cross-sensor calibration for consistency
- Validation against known concentration standards

## Data Preparation Workflow

### Single File Processing

**Step-by-Step Processing Workflow:**
Process individual video files with comprehensive normalization and validation:

```python
from src.backend.core.data_normalization import create_normalization_pipeline
from src.backend.io.video_reader import detect_video_format, create_video_reader_factory
from src.backend.utils.logging_utils import get_logger, set_scientific_context

# Setup scientific context and logging
logger = get_logger(__name__)
set_scientific_context('data_preparation_example')

# Load configuration
config = load_example_configuration('config/default_normalization.json')

# Process single file
video_path = 'data/sample_crimaldi.avi'
output_path = 'output/normalized_crimaldi.npy'

# Execute comprehensive single file processing
result = demonstrate_single_file_normalization(
    video_path=video_path,
    output_path=output_path,
    normalization_config=config,
    show_progress=True,
    enable_validation=True
)
```

**Processing Pipeline Stages:**
1. **Format Detection:** Automatic format identification with confidence scoring
2. **Metadata Extraction:** Comprehensive parameter extraction from video metadata
3. **Validation:** File integrity and parameter validation
4. **Normalization:** Multi-stage normalization pipeline execution
5. **Quality Assessment:** Comprehensive quality validation and scoring
6. **Output Generation:** Standardized output with metadata preservation

**Progress Monitoring:**
- Real-time progress bars for long-running operations
- Stage-specific timing and performance metrics
- Memory usage monitoring and optimization
- Error detection and recovery mechanisms

### Batch Processing

**Parallel Batch Processing:**
Process multiple video files simultaneously with optimal resource utilization:

```python
from src.backend.core.data_normalization import batch_normalize_plume_data
from src.backend.utils.progress_display import create_progress_bar

# Configure batch processing
batch_config = {
    'parallel_processing': True,
    'max_workers': 8,
    'memory_limit_gb': 8,
    'enable_caching': True,
    'error_handling_strategy': 'graceful_degradation'
}

# Define video files for batch processing
video_paths = [
    'data/crimaldi_sample_1.avi',
    'data/crimaldi_sample_2.avi',
    'data/custom_sample_1.avi',
    'data/custom_sample_2.avi'
]

# Execute batch normalization
batch_result = demonstrate_batch_normalization(
    video_paths=video_paths,
    output_directory='output/batch_normalized',
    batch_config=batch_config,
    enable_parallel_processing=True
)
```

**Batch Processing Features:**
- **Parallel execution:** Configurable worker processes for optimal throughput
- **Memory management:** Intelligent memory allocation and garbage collection
- **Progress tracking:** Comprehensive batch progress monitoring
- **Error handling:** Graceful degradation with detailed error reporting
- **Performance optimization:** Dynamic load balancing and resource optimization

**Batch Statistics and Reporting:**
- Success rate calculation and reporting
- Processing time analysis and optimization recommendations
- Resource utilization monitoring and bottleneck identification
- Quality assessment across batch processing results

### Cross-Format Processing

**Multi-Format Compatibility:**
Process multiple formats simultaneously with consistency validation:

```python
# Execute cross-format compatibility demonstration
compatibility_result = demonstrate_cross_format_compatibility(
    crimaldi_path='data/crimaldi_sample.avi',
    custom_path='data/custom_sample.avi',
    output_directory='output/cross_format',
    enable_comparison_analysis=True
)
```

**Cross-Format Features:**
- **Format-specific optimization:** Tailored processing parameters for each format
- **Consistency validation:** Cross-format correlation analysis and validation
- **Comparative analysis:** Detailed comparison of processing results
- **Unified output:** Standardized output format regardless of input format

**Quality Assurance for Cross-Format Processing:**
- Cross-format correlation metrics (target >95%)
- Consistency validation across different input formats
- Automated parameter adjustment for optimal cross-format compatibility
- Comprehensive quality reporting and validation

## Quality Assurance and Validation

### Validation Pipeline

**Comprehensive Validation Framework:**
The NormalizationValidator provides systematic validation across all processing stages:

```python
from src.backend.core.data_normalization.validation import NormalizationValidator

# Initialize validation framework
validation_config = {
    'quality_thresholds': {
        'spatial_accuracy': 0.01,
        'temporal_accuracy': 0.001,
        'intensity_accuracy': 0.02,
        'correlation_threshold': 0.95
    },
    'enable_fail_fast': True,
    'enable_comprehensive_validation': True
}

validator = NormalizationValidator(
    validation_config=validation_config,
    enable_caching=True,
    enable_fail_fast=True
)
```

**Validation Stages:**
1. **Configuration Validation:** Parameter consistency and constraint verification
2. **Format Validation:** Input format compatibility and conversion feasibility
3. **Processing Validation:** Real-time quality monitoring during normalization
4. **Output Validation:** Result quality assessment and correlation verification
5. **Cross-Format Validation:** Consistency validation across different input formats

### Quality Metrics

**Comprehensive Quality Assessment:**
The ValidationMetrics system provides detailed quality evaluation:

```python
from src.backend.core.data_normalization.validation import ValidationMetrics

# Initialize quality metrics calculation
metrics = ValidationMetrics(
    reference_data=reference_dataset,
    processed_data=normalized_dataset,
    validation_config=validation_config
)

# Calculate comprehensive quality metrics
quality_report = metrics.calculate_comprehensive_quality_metrics()
```

**Quality Metric Categories:**
- **Spatial Accuracy:** Geometric preservation and scaling accuracy (target >99%)
- **Temporal Consistency:** Frame rate conversion accuracy and motion preservation (target >95%)
- **Intensity Calibration:** Concentration measurement accuracy and dynamic range preservation (target >98%)
- **Overall Quality Score:** Composite quality metric (target >95% correlation)

**Quality Thresholds and Validation:**
- Automatic pass/fail determination based on quality thresholds
- Detailed quality breakdown by processing stage
- Comparative analysis against reference implementations
- Quality trend analysis for process optimization

### Performance Validation

**Processing Performance Assessment:**
Comprehensive performance validation ensures processing efficiency and scalability:

**Performance Metrics:**
- **Processing time:** Target <7.2 seconds average per simulation
- **Memory usage:** Monitor and limit to 8GB maximum
- **Throughput:** Support for 4000+ simulations within 8 hours
- **Resource utilization:** CPU and memory efficiency optimization

**Performance Monitoring:**
- Real-time performance tracking during batch processing
- Bottleneck identification and optimization recommendations
- Scaling analysis for large dataset processing
- Performance regression detection and alerting

**Quality-Performance Balance:**
- Optimization strategies that maintain quality requirements
- Adaptive processing parameters for performance optimization
- Trade-off analysis between processing speed and output quality
- Performance benchmarking against reference implementations

## Error Handling and Troubleshooting

### Common Validation Errors

**Format Validation Errors:**
Handle format compatibility and detection issues:

```python
from src.backend.error.validation_error import FormatValidationError

try:
    format_result = validator.validate_video_format(
        video_path=video_path,
        target_format='crimaldi',
        deep_validation=True
    )
except FormatValidationError as e:
    logger.error(f'Format validation failed: {e.message}')
    
    # Get detailed format analysis
    format_analysis = e.get_format_analysis()
    logger.error(f'Expected format: {e.expected_format}')
    logger.error(f'Detected format: {e.detected_format}')
    logger.error(f'Conversion possible: {e.is_format_convertible}')
    
    # Display compatibility issues
    for issue in e.format_compatibility_issues:
        logger.error(f'Compatibility issue: {issue}')
```

**Parameter Validation Errors:**
Handle calibration parameter constraint violations:

```python
from src.backend.error.validation_error import ParameterValidationError

try:
    param_result = validator.validate_calibration_parameters(
        calibration_params=calibration_params,
        source_format='crimaldi',
        target_format='normalized'
    )
except ParameterValidationError as e:
    logger.error(f'Parameter validation failed: {e.message}')
    
    # Get parameter correction suggestions
    suggested_value = e.suggest_parameter_value()
    logger.info(f'Suggested parameter correction: {suggested_value}')
    
    # Display constraint violations
    for violation in e.constraint_violations:
        logger.error(f'Constraint violation: {violation}')
```

### Fail-Fast Validation Strategy

**Early Error Detection:**
Implement fail-fast validation to prevent wasted computational resources:

```python
from src.backend.error.validation_error import ValidationError

try:
    # Enable fail-fast validation
    ValidationError.enable_fail_fast_mode(
        critical_thresholds={
            'format_compatibility': 0.8,
            'parameter_validity': 0.95,
            'file_integrity': 1.0
        }
    )
    
    # Execute validation with fail-fast triggers
    validation_result = validator.execute_comprehensive_validation(
        video_path=video_path,
        normalization_config=config,
        enable_fail_fast=True
    )
    
except ValidationError as e:
    # Trigger fail-fast response
    e.trigger_fail_fast()
    
    # Get detailed error context
    error_context = e.get_error_context()
    logger.error(f'Critical validation failure: {error_context}')
    
    # Stop processing and report failure
    return False
```

**Fail-Fast Benefits:**
- Early detection of incompatible data formats
- Prevention of wasted computational resources
- Clear error reporting with actionable recommendations
- Automatic process termination for critical issues

### Recovery Procedures

**Automatic Error Recovery:**
Implement comprehensive error recovery mechanisms:

```python
# Parameter correction and recovery
try:
    corrected_params = ParameterValidationError.auto_correct_parameters(
        invalid_params=calibration_params,
        source_format='custom_avi',
        target_constraints=validation_constraints
    )
    
    # Retry processing with corrected parameters
    retry_result = process_with_corrected_parameters(
        video_path=video_path,
        corrected_params=corrected_params,
        max_retries=3
    )
    
except Exception as e:
    # Log recovery failure and provide manual correction guidance
    logger.error(f'Automatic recovery failed: {e}')
    manual_correction_guide = generate_manual_correction_guide(e)
    logger.info(f'Manual correction required: {manual_correction_guide}')
```

**Recovery Strategies:**
- **Parameter correction:** Automatic parameter adjustment within valid ranges
- **Format conversion:** Guided format conversion for compatibility issues
- **Retry mechanisms:** Intelligent retry with exponential backoff
- **Manual intervention:** Clear guidance for manual correction when automatic recovery fails

### Performance Troubleshooting

**Performance Optimization:**
Diagnose and resolve performance bottlenecks:

```python
# Performance optimization and troubleshooting
performance_optimizer = PerformanceOptimizer(
    target_processing_time=7.2,  # seconds per simulation
    memory_limit_gb=8,
    enable_profiling=True
)

# Optimize video reader cache
optimized_cache = optimize_video_reader_cache(
    cache_size_gb=5,
    compression_method='lz4',
    enable_memory_mapping=True
)

# Tune parallel processing parameters
optimal_workers = performance_optimizer.optimize_parallel_processing(
    current_workers=8,
    memory_per_worker_gb=1,
    enable_dynamic_balancing=True
)

# Generate performance optimization report
optimization_report = performance_optimizer.generate_optimization_report()
logger.info(f'Performance optimization completed: {optimization_report}')
```

**Performance Optimization Areas:**
- **Cache optimization:** Intelligent caching strategies for improved throughput
- **Memory management:** Optimal memory allocation and garbage collection
- **Parallel processing:** Dynamic load balancing and worker optimization
- **I/O optimization:** Efficient disk access patterns and memory mapping

## Advanced Configuration

### Custom Format Configuration

**Custom Format Support:**
Configure support for specialized or proprietary video formats:

```json
{
  "custom_format_detection": {
    "enable_auto_detection": true,
    "confidence_threshold": 0.8,
    "metadata_extraction_methods": ["file_header", "embedded_data", "filename_parsing"],
    "calibration_parameter_sources": ["metadata", "configuration_file", "user_input"]
  },
  "format_specific_optimizations": {
    "h264_optimization": {
      "decode_threads": 4,
      "hardware_acceleration": "auto",
      "memory_buffering": "optimized"
    },
    "mjpeg_optimization": {
      "frame_caching": true,
      "random_access_optimization": true,
      "quality_preservation": "high"
    }
  }
}
```

**Advanced Format Features:**
- Auto-detection parameter tuning for improved format recognition
- Custom calibration requirement specification
- Metadata extraction configuration for various sources
- Format-specific optimization settings for improved performance

### Performance Optimization

**Advanced Performance Configuration:**
Configure sophisticated performance optimization strategies:

```json
{
  "memory_management": {
    "memory_limit_mb": 8192,
    "memory_mapping": true,
    "garbage_collection_strategy": "generational",
    "buffer_pool_size_mb": 1024
  },
  "parallel_processing": {
    "max_workers": 8,
    "dynamic_load_balancing": true,
    "work_stealing": true,
    "numa_awareness": true
  },
  "caching_optimization": {
    "cache_size_limit_gb": 5,
    "compression": "lz4",
    "cache_eviction_policy": "lru",
    "preload_strategies": ["predictive", "sequential"]
  }
}
```

**Performance Optimization Features:**
- Intelligent memory management with configurable limits and strategies
- Advanced parallel processing with dynamic load balancing
- Sophisticated caching with compression and intelligent eviction
- NUMA-aware processing for multi-socket systems

### Scientific Computing Precision

**Precision Configuration:**
Configure precision requirements for scientific computing accuracy:

```json
{
  "numerical_precision": {
    "floating_point_precision": "float64",
    "numerical_accuracy_threshold": 1e-06,
    "error_propagation_analysis": true,
    "reproducibility_requirements": "strict"
  },
  "correlation_requirements": {
    "minimum_correlation": 0.95,
    "statistical_significance": 0.01,
    "cross_validation_enabled": true,
    "reference_implementation_validation": true
  },
  "scientific_validation": {
    "enable_statistical_tests": true,
    "confidence_intervals": 0.95,
    "outlier_detection": "robust",
    "reproducibility_testing": true
  }
}
```

**Scientific Computing Features:**
- High-precision numerical computation with configurable accuracy thresholds
- Statistical validation and significance testing
- Reproducibility requirements for research compliance
- Cross-validation against reference implementations

## Examples and Best Practices

### Complete Processing Example

**Comprehensive End-to-End Example:**
Demonstrate complete data preparation workflow with all features:

```python
def run_comprehensive_example():
    """
    Complete example demonstrating all data preparation capabilities
    including single file processing, batch operations, cross-format
    compatibility, performance optimization, and quality validation.
    """
    
    # Setup scientific context and logging
    logger = get_logger(__name__)
    set_scientific_context('comprehensive_data_preparation_example')
    
    try:
        # Load comprehensive configuration
        config = load_example_configuration('config/default_normalization.json')
        
        # Initialize validation framework
        validator = NormalizationValidator(
            validation_config=config['validation'],
            enable_fail_fast=True,
            enable_caching=True
        )
        
        # Single file processing example
        logger.info("Starting single file processing example...")
        single_result = demonstrate_single_file_processing(
            video_path='data/sample_crimaldi.avi',
            output_path='output/single_normalized.npy',
            config=config,
            validator=validator
        )
        
        # Batch processing example
        logger.info("Starting batch processing example...")
        batch_result = demonstrate_batch_processing(
            video_paths=['data/batch_sample_1.avi', 'data/batch_sample_2.avi'],
            output_directory='output/batch_normalized',
            config=config,
            validator=validator
        )
        
        # Cross-format compatibility example
        logger.info("Starting cross-format compatibility example...")
        compatibility_result = demonstrate_cross_format_compatibility(
            crimaldi_path='data/crimaldi_sample.avi',
            custom_path='data/custom_sample.avi',
            output_directory='output/cross_format',
            config=config,
            validator=validator
        )
        
        # Performance optimization example
        logger.info("Starting performance optimization example...")
        performance_result = demonstrate_performance_optimization(
            config=config,
            target_processing_time=7.2,
            memory_limit_gb=8
        )
        
        # Generate comprehensive report
        comprehensive_report = generate_comprehensive_report([
            single_result, batch_result, compatibility_result, performance_result
        ])
        
        logger.info("Comprehensive example completed successfully")
        logger.info(f"Overall quality score: {comprehensive_report['overall_quality']:.4f}")
        logger.info(f"Processing efficiency: {comprehensive_report['efficiency']:.2%}")
        
        return comprehensive_report
        
    except Exception as e:
        logger.error(f"Comprehensive example failed: {e}")
        error_analysis = analyze_processing_error(e)
        logger.error(f"Error analysis: {error_analysis}")
        return None
```

### Configuration Best Practices

**Optimal Configuration Guidelines:**
Recommendations for achieving optimal processing results:

**Parameter Selection Best Practices:**
- **Arena normalization:** Use standard 1.0m x 1.0m target for consistency
- **Resolution normalization:** Balance quality and performance with 640x480 target
- **Frame rate:** Standardize to 30.0 fps for optimal processing speed
- **Quality thresholds:** Maintain >95% correlation requirement for scientific accuracy

**Performance Tuning Recommendations:**
- **Memory allocation:** Use 8GB limit with memory mapping enabled
- **Parallel processing:** Match worker count to available CPU cores
- **Caching:** Enable 5GB cache with LZ4 compression for optimal I/O
- **Validation:** Use fail-fast strategy for early error detection

**Cross-Format Compatibility Considerations:**
- Configure format-specific optimization for each supported format
- Use adaptive processing parameters based on detected format characteristics
- Enable comprehensive validation for cross-format consistency
- Maintain quality thresholds across all format conversions

### Validation Best Practices

**Comprehensive Validation Strategy:**
Implement robust validation procedures for reliable scientific computing:

**Validation Pipeline Best Practices:**
1. **Pre-processing validation:** Comprehensive format and parameter validation
2. **Real-time monitoring:** Continuous quality assessment during processing
3. **Post-processing validation:** Thorough result validation and correlation checking
4. **Cross-validation:** Validation against reference implementations and known benchmarks

**Quality Metrics Interpretation:**
- **Spatial accuracy >99%:** Indicates excellent geometric preservation
- **Temporal consistency >95%:** Confirms proper motion preservation
- **Intensity accuracy >98%:** Validates concentration measurement preservation
- **Overall correlation >95%:** Meets scientific computing requirements

**Error Handling Best Practices:**
- Implement fail-fast validation for critical errors
- Use graceful degradation for non-critical issues
- Provide detailed error context and recovery recommendations
- Maintain comprehensive audit trails for debugging and validation

**Performance Monitoring Best Practices:**
- Monitor processing time against 7.2-second target per simulation
- Track memory usage to prevent system resource exhaustion
- Validate throughput capability for 4000+ simulation requirements
- Implement performance regression detection and alerting

## Reference Information

### Configuration Reference

**Complete Parameter Reference:**
Comprehensive documentation of all configuration parameters:

**Core Pipeline Parameters:**
- `enable_parallel_processing`: Boolean, default true - Enable parallel processing
- `max_workers`: Integer, default 8 - Maximum parallel worker processes
- `memory_limit_gb`: Float, default 8.0 - Memory usage limit in gigabytes
- `enable_caching`: Boolean, default true - Enable intermediate result caching
- `processing_precision`: String, default "float64" - Numerical precision level

**Arena Normalization Parameters:**
- `target_arena_width_meters`: Float, default 1.0 - Target arena width
- `target_arena_height_meters`: Float, default 1.0 - Target arena height
- `scaling_method`: String, default "bicubic" - Scaling interpolation method
- `boundary_detection`: String, default "edge_detection" - Boundary detection method
- `validation_accuracy_threshold`: Float, default 0.01 - Accuracy threshold

**Quality Validation Parameters:**
- `spatial_accuracy`: Float, default 0.01 - Spatial accuracy threshold
- `temporal_accuracy`: Float, default 0.001 - Temporal accuracy threshold
- `intensity_accuracy`: Float, default 0.02 - Intensity accuracy threshold
- `correlation_threshold`: Float, default 0.95 - Minimum correlation requirement

### API Reference

**Core Data Preparation Functions:**

```python
def create_video_processor(config_path: str, enable_caching: bool = True) -> VideoProcessor:
    """
    Create video processor with specified configuration.
    
    Args:
        config_path: Path to normalization configuration file
        enable_caching: Enable intermediate result caching
        
    Returns:
        VideoProcessor: Configured video processing instance
    """

def process_video_file(video_path: str, output_path: str, config: dict) -> ProcessingResult:
    """
    Process single video file with normalization pipeline.
    
    Args:
        video_path: Input video file path
        output_path: Output normalized data path
        config: Normalization configuration dictionary
        
    Returns:
        ProcessingResult: Processing result with quality metrics
    """

def process_video_batch(video_paths: List[str], output_dir: str, config: dict) -> BatchResult:
    """
    Process multiple video files in parallel batch operation.
    
    Args:
        video_paths: List of input video file paths
        output_dir: Output directory for normalized data
        config: Batch processing configuration
        
    Returns:
        BatchResult: Batch processing results and statistics
    """

def validate_video_processing_quality(processed_data: np.ndarray, reference_data: np.ndarray) -> QualityMetrics:
    """
    Validate processing quality against reference implementation.
    
    Args:
        processed_data: Normalized video data array
        reference_data: Reference implementation results
        
    Returns:
        QualityMetrics: Comprehensive quality assessment metrics
    """
```

### Error Code Reference

**Comprehensive Error Code Documentation:**

**Validation Error Codes (1100-1107):**
- **1100 - FormatValidationError:** Video format incompatibility or detection failure
- **1101 - ParameterValidationError:** Calibration parameter constraint violation
- **1102 - SchemaValidationError:** Configuration file structure or content error
- **1103 - CrossFormatValidationError:** Cross-format compatibility validation failure
- **1104 - QualityValidationError:** Processing quality below required thresholds
- **1105 - PerformanceValidationError:** Processing performance requirements not met
- **1106 - IntegrityValidationError:** File integrity or corruption detection
- **1107 - CalibrationValidationError:** Calibration parameter accuracy validation failure

**Error Recovery Strategies:**
Each error code includes specific recovery recommendations:
- Automatic parameter correction where possible
- Format conversion guidance for compatibility issues
- Performance optimization recommendations
- Manual intervention procedures for critical failures

**Troubleshooting Support:**
- Detailed error context and diagnostic information
- Step-by-step recovery procedures
- Common issue resolution guides
- Performance optimization recommendations

---

This comprehensive data preparation guide provides detailed instructions for achieving reliable, scientific-grade processing of plume recording data with >95% correlation accuracy and support for 4000+ simulation processing requirements. Follow these procedures for optimal results in your plume simulation research.